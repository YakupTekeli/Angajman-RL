import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import time
import json
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')

from Environment import SwarmBattlefield2D
from TrainLoop import HierarchicalSwarmTrainer
from Visualization import SwarmVisualization
from SwarmCoordinator import SwarmCoordinator  # YENÄ°!


class SwarmTrainingManager:
    def __init__(self, config_path=None):
        self.config = self._load_config(config_path)
        self.env = None
        self.trainer = None
        self.coordinator = None  # YENÄ°!
        self.visualizer = SwarmVisualization()

        # SonuÃ§ dizinleri
        self.base_dir = "swarm_training_results"
        self.create_directories()

        # KayÄ±tlar
        self.training_log = []
        self.start_time = None

        print("=" * 70)
        print("FPV KAMÄ°KAZE SÃœRÃœSÃœ RL EÄÄ°TÄ°M SÄ°STEMÄ°")
        print("ğŸ¯ SÃœRÃœ MUHAKEME VE KOORDÄ°NASYON SÄ°STEMÄ° AKTÄ°F")
        print("=" * 70)

    def _load_config(self, config_path):
        """KonfigÃ¼rasyon yÃ¼kle - GÃœNCELLENMÄ°Å"""
        default_config = {
            # Ortam parametreleri
            'env': {
                'width': 1200,
                'height': 800,
                'num_drones': 6,
                'num_targets': 12,
                'max_steps': 1000
            },

            # GELÄ°ÅTÄ°RÄ°LMÄ°Å eÄŸitim parametreleri
            'training': {
                'total_episodes': 1000,  # ARTIRILDI
                'batch_size': 64,  # AZALTILDI
                'gamma': 0.99,
                'learning_rate': 0.0001,  # AZALTILDI
                'epsilon_start': 1.0,
                'epsilon_end': 0.1,  # ARTIRILDI
                'epsilon_decay': 0.996,  # SÃœPER YAVASLATILDI (SabÄ±r YamasÄ±)
                'tau': 0.001,  # YAVASLATILDI
                'buffer_size': 20000,  # ARTIRILDI
                'save_interval': 50,
                'eval_interval': 20,
                'render_interval': 100
            },

            # GELÄ°ÅTÄ°RÄ°LMÄ°Å curriculum learning
            'curriculum': {
                'enabled': True,
                'stages': [
                    # Ã‡ok kolay baÅŸla - Drone sayÄ±sÄ± sabit, hedef artar
                    {'episodes': 300, 'num_targets': 3, 'width': 600, 'height': 400},
                    {'episodes': 300, 'num_targets': 6, 'width': 800, 'height': 600},
                    {'episodes': 250, 'num_targets': 9, 'width': 1000, 'height': 700},
                    {'episodes': 500, 'num_targets': 12, 'width': 1200, 'height': 800}
                ]
            },

            # ğŸ¯ YENÄ°: Koordinasyon ayarlarÄ±
            'coordination': {
                'enabled': True,  # KoordinatÃ¶rÃ¼ kullan
                'verbose': True  # KoordinatÃ¶r loglarÄ±nÄ± gÃ¶ster
            }
        }

        if config_path and os.path.exists(config_path):
            with open(config_path, 'r') as f:
                user_config = json.load(f)
                import copy
                merged_config = copy.deepcopy(default_config)
                self._deep_update(merged_config, user_config)
                return merged_config

        return default_config

    def _deep_update(self, target, source):
        """Deep dictionary update"""
        for key, value in source.items():
            if isinstance(value, dict) and key in target:
                self._deep_update(target[key], value)
            else:
                target[key] = value

    def create_directories(self):
        """Dizinleri oluÅŸtur"""
        dirs = ['models', 'logs', 'plots', 'dashboards', 'videos']
        for dir_name in dirs:
            os.makedirs(os.path.join(self.base_dir, dir_name), exist_ok=True)

    def setup_environment(self, stage_config=None):
        """OrtamÄ± kur"""
        env_config = self.config['env'].copy()

        if stage_config:
            for k, v in stage_config.items():
                if k in ['width', 'height', 'num_targets', 'max_steps']:
                    env_config[k] = v

        self.env = SwarmBattlefield2D(
            width=env_config['width'],
            height=env_config['height'],
            num_drones=env_config['num_drones'],
            num_targets=env_config['num_targets']
        )

        self.env.max_steps = env_config.get('max_steps', 1000)

        # ğŸ¯ KOORDÄ°NATÃ–RÃœ KURU
        if self.config['coordination']['enabled']:
            self.coordinator = SwarmCoordinator(self.env)
            print(f"[SETUP] âœ… SÃ¼rÃ¼ koordinatÃ¶rÃ¼ aktif!")
        else:
            self.coordinator = None
            print(f"[SETUP] âš ï¸  KoordinatÃ¶r kapalÄ± - baÄŸÄ±msÄ±z drone'lar")

        print(f"[SETUP] Ortam: {env_config['width']}x{env_config['height']}")
        print(f"[SETUP] {env_config['num_drones']} drone, {env_config['num_targets']} hedef")

    def setup_trainer(self):
        """EÄŸitmeni kur"""
        self.trainer = HierarchicalSwarmTrainer(
            self.env,
            self.config['training']
        )

        print(f"[SETUP] EÄŸitmen oluÅŸturuldu")
        print(f"[SETUP] State dim: {self.trainer.state_dim} (koordinasyon dahil)")

    def evaluate_policy(self, num_episodes=5, render=False):
        """PolitikayÄ± deÄŸerlendir - KOORDÄ°NASYONLU"""
        print(f"\n[EVAL] Politik deÄŸerlendirme ({num_episodes} episode)...")

        eval_rewards = []
        eval_success_rates = []

        for eval_ep in range(num_episodes):
            observations = self.env.reset()

            if self.coordinator:
                self.coordinator.reset()

            episode_reward = 0
            done = False

            if render:
                self.env.render()

            while not done:
                # KoordinatÃ¶rden direktif al
                directives = None
                if self.coordinator:
                    directives = self.coordinator.get_strategic_actions(observations)

                actions = []

                for drone_id, obs in enumerate(observations):
                    if obs.get('health', 1.0) <= 0 or obs.get('battery', 0) <= 0:
                        actions.append([0.0, 0.0, 0, -1])
                        continue

                    # Direktif
                    directive = directives[drone_id] if directives else None

                    # State + direktif
                    state = self.trainer._process_observation(obs, directive)
                    action, _, _, _ = self.trainer.agents[drone_id].get_action(state, epsilon=0.0)

                    move_x = float(action[0])
                    move_y = float(action[1])
                    attack = int(action[2])
                    target_id = -1

                    # KoordinatÃ¶rden hedef al
                    if directive and directive.get('target_id', -1) >= 0:
                        target_id = directive['target_id']
                        if directive.get('should_attack', False):
                            attack = 1
                    else:
                        # Fallback
                        if attack == 1:
                            visible = obs.get('visible_targets', [])
                            if isinstance(visible, list) and len(visible) > 0:
                                def target_key(t):
                                    imp = float(t.get('importance', 0.0))
                                    dist = float(t.get('distance', 1.0))
                                    return (imp, - (1.0 - dist))

                                best = max(visible, key=target_key)
                                target_id = int(best.get('id', -1))

                    actions.append([move_x, move_y, attack, target_id])

                observations, rewards, done, info = self.env.step(actions)
                episode_reward += sum(rewards)

                if render:
                    self.env.render()
                    time.sleep(0.01)

            eval_rewards.append(episode_reward)
            eval_success_rates.append(info.get('success_rate', 0))

            print(f"[EVAL] Episode {eval_ep + 1}: Ã–dÃ¼l={episode_reward:.2f}, "
                  f"BaÅŸarÄ±={info.get('success_rate', 0):.1f}%")

        avg_reward = np.mean(eval_rewards)
        avg_success = np.mean(eval_success_rates)

        print(f"[EVAL] Ortalama Ã–dÃ¼l: {avg_reward:.2f}")
        print(f"[EVAL] Ortalama BaÅŸarÄ±: {avg_success:.1f}%")

        return avg_reward, avg_success

    def run_training(self):
        """Ana eÄŸitim dÃ¶ngÃ¼sÃ¼"""
        print("\n" + "=" * 70)
        print("EÄÄ°TÄ°M BAÅLIYOR")
        if self.config['coordination']['enabled']:
            print("ğŸ¯ SÃœRÃœ KOORDÄ°NASYONU AKTÄ°F")
        print("=" * 70)

        self.start_time = time.time()

        # Curriculum learning
        if self.config['curriculum']['enabled']:
            self._run_curriculum_training()
        else:
            self._run_standard_training()

        # Final deÄŸerlendirme
        print("\n" + "=" * 70)
        print("FINAL DEÄERLENDÄ°RME")
        print("=" * 70)

        final_reward, final_success = self.evaluate_policy(num_episodes=10, render=False)

        # Final dashboard
        self.create_final_dashboard(final_reward, final_success)

        # EÄŸitim Ã¶zeti
        self.print_training_summary()

    def _run_curriculum_training(self):
        """Curriculum learning ile eÄŸitim"""
        stages = self.config['curriculum']['stages']
        total_episodes = 0
        fixed_num_drones = self.config['env']['num_drones']

        for stage_idx, stage in enumerate(stages):
            print(f"\n{'=' * 60}")
            print(f"AÅAMA {stage_idx + 1}/{len(stages)}")
            print(f"{'=' * 60}")
            print(f"Episode: {stage['episodes']}")
            print(f"Drone: {fixed_num_drones}, Hedef: {stage['num_targets']}")
            print(f"Harita: {stage['width']}x{stage['height']}")

            # OrtamÄ± kur
            self.setup_environment(stage)

            # Ä°lk aÅŸamada trainer oluÅŸtur
            if stage_idx == 0:
                self.setup_trainer()
            else:
                self.trainer.env = self.env
                # KoordinatÃ¶rÃ¼ de gÃ¼ncelle
                if self.coordinator:
                    self.coordinator.env = self.env

            # AÅŸama eÄŸitimi
            stage_start = self.trainer.episode
            stage_end = stage_start + stage['episodes']

            while self.trainer.episode < stage_end:
                self._train_single_episode()
                total_episodes += 1

            print(f"[STAGE] AÅŸama {stage_idx + 1} tamamlandÄ±")

            # AÅŸama sonu deÄŸerlendirme
            if stage_idx < len(stages) - 1:
                avg_reward, avg_success = self.evaluate_policy(num_episodes=3)
                print(f"[STAGE] AÅŸama {stage_idx + 1} deÄŸerlendirmesi:")
                print(f"       Ortalama Ã–dÃ¼l: {avg_reward:.2f}")
                print(f"       Ortalama BaÅŸarÄ±: {avg_success:.1f}%")

    def _run_standard_training(self):
        """Standart eÄŸitim"""
        self.setup_environment()
        self.setup_trainer()

        total_episodes = self.config['training']['total_episodes']

        for ep in range(total_episodes):
            self._train_single_episode()

    def _train_single_episode(self):
        """Tek episode eÄŸit - KOORDÄ°NASYONLU"""
        episode_num = self.trainer.episode + 1

        # ğŸ¯ KOORDÄ°NATÃ–RÃœ KULLANARAK EÄÄ°T
        episode_reward, info = self.trainer.train_episode(coordinator=self.coordinator)

        # KayÄ±t
        log_entry = {
            'episode': episode_num,
            'reward': episode_reward,
            'success_rate': info.get('success_rate', 0),
            'destroyed_targets': info.get('destroyed_targets', 0),
            'destroyed_drones': info.get('destroyed_drones', 0),
            'epsilon': self.trainer.epsilon,
            'timestamp': datetime.now().isoformat()
        }

        # KoordinatÃ¶r bilgilerini ekle
        if self.coordinator:
            coord_summary = self.coordinator.get_mission_summary()
            log_entry['coordination'] = coord_summary

        self.training_log.append(log_entry)

        # Periyodik iÅŸlemler
        save_interval = self.config['training']['save_interval']
        eval_interval = self.config['training']['eval_interval']
        render_interval = self.config['training']['render_interval']

        # Model kaydet
        if episode_num % save_interval == 0:
            model_path = os.path.join(self.base_dir, 'models')
            self.trainer.save_model(model_path)

            # Log kaydet
            log_path = os.path.join(self.base_dir, 'logs', f'training_log_ep{episode_num}.json')
            with open(log_path, 'w') as f:
                json.dump(self.training_log, f, indent=2)

            print(f"[SAVE] Model ve log kaydedildi (Episode {episode_num})")

        # DeÄŸerlendirme
        if episode_num % eval_interval == 0:
            eval_reward, eval_success = self.evaluate_policy(num_episodes=2)

            # History'e kaydet
            self.trainer.history.setdefault('eval_rewards', []).append(eval_reward)
            self.trainer.history.setdefault('eval_success', []).append(eval_success)

            print(f"[EVAL] Episode {episode_num}: Eval Ã–dÃ¼l={eval_reward:.2f}, "
                  f"Eval BaÅŸarÄ±={eval_success:.1f}%")

        # GÃ¶rselleÅŸtirme
        if episode_num % render_interval == 0:
            dashboard = self.visualizer.create_training_dashboard(
                self.trainer.history,
                current_episode=episode_num
            )

            dashboard_path = os.path.join(self.base_dir, 'dashboards',
                                          f'dashboard_ep{episode_num:04d}.png')
            dashboard.savefig(dashboard_path, dpi=150, bbox_inches='tight')
            plt.close(dashboard)

            print(f"[VIZ] Dashboard kaydedildi: {dashboard_path}")

    def create_final_dashboard(self, final_reward, final_success):
        """Final dashboard oluÅŸtur"""
        print("\n[FINAL] Final dashboard oluÅŸturuluyor...")

        # 1. Training dashboard
        training_fig = self.visualizer.create_training_dashboard(
            self.trainer.history,
            current_episode=self.trainer.episode
        )

        training_path = os.path.join(self.base_dir, 'plots', 'training_dashboard.png')
        training_fig.savefig(training_path, dpi=200, bbox_inches='tight')
        plt.close(training_fig)

        # 2. Performance comparison
        comparison_fig = self.visualizer.create_performance_comparison(self.trainer.history)
        comparison_path = os.path.join(self.base_dir, 'plots', 'performance_comparison.png')
        comparison_fig.savefig(comparison_path, dpi=200, bbox_inches='tight')
        plt.close(comparison_fig)

        # 3. Interactive dashboard
        interactive_fig = self.visualizer.create_interactive_dashboard(self.trainer.history)
        if interactive_fig:
            interactive_path = os.path.join(self.base_dir, 'dashboards', 'interactive_dashboard.html')
            interactive_fig.write_html(interactive_path)

        # 4. Final summary
        self._create_final_summary_plot(final_reward, final_success)

        print(f"[FINAL] Dashboard'lar kaydedildi: {self.base_dir}/plots/")

    def _create_final_summary_plot(self, final_reward, final_success):
        """Final Ã¶zet grafiÄŸi"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. Ã–dÃ¼l trendi
        if 'episode_rewards' in self.trainer.history:
            rewards = self.trainer.history['episode_rewards']
            episodes = range(len(rewards))

            axes[0, 0].plot(episodes, rewards, 'b-', alpha=0.5, linewidth=1)

            if len(rewards) > 10:
                window = min(50, len(rewards))
                moving_avg = pd.Series(rewards).rolling(window=window).mean()
                axes[0, 0].plot(episodes[window - 1:], moving_avg[window - 1:],
                                'r-', linewidth=2, label=f'{window} Ep. Ort.')

            axes[0, 0].set_title('Ã–ÄŸrenme EÄŸrisi', fontsize=14, fontweight='bold')
            axes[0, 0].set_xlabel('Episode')
            axes[0, 0].set_ylabel('Ã–dÃ¼l')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)

        # 2. BaÅŸarÄ± oranÄ±
        if 'success_rate' in self.trainer.history:
            success = self.trainer.history['success_rate']
            episodes = range(len(success))

            axes[0, 1].plot(episodes, success, 'g-', linewidth=2)
            axes[0, 1].fill_between(episodes, success, alpha=0.3, color='green')
            axes[0, 1].axhline(y=final_success, color='r', linestyle='--',
                               linewidth=2, label=f'Final: {final_success:.1f}%')

            axes[0, 1].set_title('BaÅŸarÄ± OranÄ± GeliÅŸimi', fontsize=14, fontweight='bold')
            axes[0, 1].set_xlabel('Episode')
            axes[0, 1].set_ylabel('BaÅŸarÄ± %')
            axes[0, 1].set_ylim(0, 100)
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)

        # 3. Hedef baÅŸarÄ± oranlarÄ±
        target_types = ['tank', 'artillery', 'infantry', 'aircraft', 'radar']
        target_colors = ['darkgreen', 'brown', 'lightblue', 'gray', 'orange']

        final_rates = []
        for target_type in target_types:
            rate_key = f'{target_type}_rate'
            if rate_key in self.trainer.history and len(self.trainer.history[rate_key]) > 0:
                final_rates.append(self.trainer.history[rate_key][-1])
            else:
                final_rates.append(0)

        bars = axes[1, 0].bar(target_types, final_rates, color=target_colors, alpha=0.8)
        axes[1, 0].set_title('Hedef Tipi BaÅŸarÄ± OranlarÄ±', fontsize=14, fontweight='bold')
        axes[1, 0].set_ylabel('BaÅŸarÄ± %')
        axes[1, 0].set_ylim(0, 100)

        for bar, rate in zip(bars, final_rates):
            height = bar.get_height()
            axes[1, 0].text(bar.get_x() + bar.get_width() / 2., height + 2,
                            f'{rate:.1f}%', ha='center', va='bottom', fontsize=10)

        # 4. Ã–zet metni
        axes[1, 1].axis('off')

        training_time = time.time() - self.start_time
        hours = int(training_time // 3600)
        minutes = int((training_time % 3600) // 60)
        seconds = int(training_time % 60)

        summary_text = "EÄÄ°TÄ°M Ã–ZETÄ°\n"
        summary_text += "=" * 40 + "\n\n"

        if self.config['coordination']['enabled']:
            summary_text += "ğŸ¯ SÃœRÃœ KOORDÄ°NASYONU: AKTÄ°F\n\n"

        summary_text += f"Toplam Episode: {self.trainer.episode}\n"
        summary_text += f"Toplam AdÄ±m: {self.trainer.total_steps}\n"
        summary_text += f"EÄŸitim SÃ¼resi: {hours:02d}:{minutes:02d}:{seconds:02d}\n\n"

        if 'episode_rewards' in self.trainer.history:
            rewards = self.trainer.history['episode_rewards']
            summary_text += f"Ortalama Ã–dÃ¼l: {np.mean(rewards):.2f}\n"
            summary_text += f"En Ä°yi Ã–dÃ¼l: {np.max(rewards):.2f}\n"
            summary_text += f"Son Ã–dÃ¼l: {rewards[-1]:.2f}\n\n"

        summary_text += f"Final Ã–dÃ¼l: {final_reward:.2f}\n"
        summary_text += f"Final BaÅŸarÄ±: {final_success:.1f}%\n\n"

        summary_text += "PERFORMANS:\n"
        if final_success > 70:
            summary_text += "âœ… MÃœKEMMEL! SÃ¼rÃ¼ koordineli Ã§alÄ±ÅŸÄ±yor.\n"
        elif final_success > 40:
            summary_text += "ğŸ‘ Ä°YÄ°! Koordinasyon geliÅŸiyor.\n"
        elif final_success > 15:
            summary_text += "âš ï¸  ORTA! Daha fazla eÄŸitim gerekli.\n"
        else:
            summary_text += "âŒ DÃœÅÃœK! Parametre ayarÄ± gerekli.\n"

        axes[1, 1].text(0.05, 0.95, summary_text, transform=axes[1, 1].transAxes,
                        fontsize=10, verticalalignment='top',
                        bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))

        plt.suptitle('FPV Kamikaze SÃ¼rÃ¼sÃ¼ - Koordinasyonlu RL EÄŸitimi',
                     fontsize=18, fontweight='bold', y=1.02)

        plt.tight_layout()

        final_plot_path = os.path.join(self.base_dir, 'plots', 'final_summary.png')
        plt.savefig(final_plot_path, dpi=200, bbox_inches='tight')
        plt.close()

    def print_training_summary(self):
        """EÄŸitim Ã¶zetini yazdÄ±r"""
        print("\n" + "=" * 70)
        print("EÄÄ°TÄ°M Ã–ZETÄ°")
        print("=" * 70)

        training_time = time.time() - self.start_time
        hours = int(training_time // 3600)
        minutes = int(training_time % 3600 // 60)
        seconds = int(training_time % 60)

        print(f"\nâ±ï¸  Toplam SÃ¼re: {hours:02d}:{minutes:02d}:{seconds:02d}")
        print(f"ğŸ“Š Toplam Episode: {self.trainer.episode}")
        print(f"ğŸ‘£ Toplam AdÄ±m: {self.trainer.total_steps}")

        if 'episode_rewards' in self.trainer.history:
            rewards = self.trainer.history['episode_rewards']
            print(f"\nğŸ’° Ã–dÃ¼l Ä°statistikleri:")
            print(f"   Ortalama: {np.mean(rewards):.2f} Â± {np.std(rewards):.2f}")
            print(f"   En Ä°yi: {np.max(rewards):.2f}")
            print(f"   Son: {rewards[-1]:.2f}")

        if 'success_rate' in self.trainer.history:
            success = self.trainer.history['success_rate']
            print(f"\nğŸ¯ BaÅŸarÄ± Ä°statistikleri:")
            print(f"   Ortalama: {np.mean(success):.1f}%")
            print(f"   En Ä°yi: {np.max(success):.1f}%")
            print(f"   Son: {success[-1]:.1f}%")

        if self.config['coordination']['enabled']:
            print(f"\nğŸ¯ Koordinasyon: AKTÄ°F âœ…")

        print(f"\nğŸ“ SonuÃ§lar: {self.base_dir}/")
        print(f"   ğŸ“Š Grafikler: {self.base_dir}/plots/")
        print(f"   ğŸ’¾ Modeller: {self.base_dir}/models/")
        print(f"   ğŸ“ Loglar: {self.base_dir}/logs/")

        print("\n" + "=" * 70)
        print("EÄÄ°TÄ°M TAMAMLANDI! ğŸ‰")
        print("=" * 70)


def main():
    """Ana fonksiyon"""
    print("FPV Kamikaze SÃ¼rÃ¼sÃ¼ RL EÄŸitim Sistemi")
    print("Version: 3.0 - SÃ¼rÃ¼ Koordinasyonu")

    config_path = None

    # Training manager oluÅŸtur
    manager = SwarmTrainingManager(config_path)

    # EÄŸitimi baÅŸlat
    try:
        manager.run_training()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  EÄŸitim durduruldu!")
        print("   Model kaydediliyor...")

        if manager.trainer:
            model_path = os.path.join(manager.base_dir, 'models', 'interrupted_model.pth')
            manager.trainer.save_model(os.path.dirname(model_path))

            if manager.trainer.history:
                dashboard = manager.visualizer.create_training_dashboard(
                    manager.trainer.history,
                    current_episode=manager.trainer.episode
                )
                dashboard_path = os.path.join(manager.base_dir, 'plots', 'interrupted_dashboard.png')
                dashboard.savefig(dashboard_path, dpi=150, bbox_inches='tight')
                plt.close(dashboard)

        print(f"   Kaydedildi: {manager.base_dir}/")
    except Exception as e:
        print(f"\n\nâŒ Hata: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()